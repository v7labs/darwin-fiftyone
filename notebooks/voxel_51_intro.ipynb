{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne Core Concepts\n",
    "\n",
    "This notebook covers the core concepts of FiftyOne, that are relevant to V7. For full information, please refer to [Voxel51's documentation](https://docs.voxel51.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The `Dataset` class is the most fundamental data structure in FiftyOne. It allows exploration & manipulation of all data relating to a specific dataset, including loading files and annotations into the dataset. Datasets are ordered collections of `Sample` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        my-dataset\n",
      "Media type:  None\n",
      "Num samples: 0\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Create an empty dataset\n",
    "dataset = fo.Dataset(\"my-dataset\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are homogenous in that all their samples must be of the same media type. The media type of a dataset is set by the file extension of the first sample added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "image\n"
     ]
    }
   ],
   "source": [
    "dataset = fo.Dataset()\n",
    "\n",
    "print(dataset.media_type)\n",
    "# None\n",
    "\n",
    "sample = fo.Sample(filepath=\"/path/to/image.png\")\n",
    "dataset.add_sample(sample)\n",
    "\n",
    "print(dataset.media_type)\n",
    "# \"image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n",
    "\n",
    "A `Sample` object stores information related to a given piece of data (image or video). Every sample is initialized with a `filepath` pointing to the relevant data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " 100% |█████████████████████| 3/3 [11.6ms elapsed, 0s remaining, 257.5 samples/s]    \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# An image sample\n",
    "sample = fo.Sample(filepath=\"/path/to/image.png\")\n",
    "\n",
    "# A video sample\n",
    "another_sample = fo.Sample(filepath=\"/path/to/video.mp4\")\n",
    "\n",
    "# Adding samples to a dataset\n",
    "dataset = fo.Dataset()\n",
    "print(len(dataset))\n",
    "# 0\n",
    "\n",
    "dataset.add_samples(\n",
    "    [\n",
    "        fo.Sample(filepath=\"/path/to/image1.jpg\"),\n",
    "        fo.Sample(filepath=\"/path/to/image2.jpg\"),\n",
    "        fo.Sample(filepath=\"/path/to/image3.jpg\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(len(dataset))\n",
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field\n",
    "\n",
    "A `Field` is an attribute of a `Sample` that stores some information. Fields can be created, modified, and deleted on a per-`Sample` basis. All samples have some required fields by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sample: {\n",
      "    'id': None,\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/path/to/image.png',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "    'created_at': None,\n",
      "    'last_modified_at': None,\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "sample = fo.Sample(filepath=\"/path/to/image.png\")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels\n",
    "\n",
    "Labels store annotation data about a particular `Sample`. FiftyOne provides many label types, including Classifications, Detections, Keypoints, and Polylines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Polyline: {\n",
      "    'id': '67b5e61ee9aaca4b31d78259',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': None,\n",
      "    'points': [[[0.3, 0.3], [0.7, 0.3], [0.7, 0.3]]],\n",
      "    'confidence': None,\n",
      "    'index': None,\n",
      "    'closed': False,\n",
      "    'filled': False,\n",
      "}>\n",
      "<Polyline: {\n",
      "    'id': '67b5e61ee9aaca4b31d7825a',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'triangle',\n",
      "    'points': [[[0.1, 0.1], [0.3, 0.1], [0.3, 0.3]]],\n",
      "    'confidence': None,\n",
      "    'index': None,\n",
      "    'closed': True,\n",
      "    'filled': True,\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "sample = fo.Sample(filepath=\"/path/to/image.png\")\n",
    "\n",
    "# A simple polyline\n",
    "polyline1 = fo.Polyline(\n",
    "    points=[[(0.3, 0.3), (0.7, 0.3), (0.7, 0.3)]],\n",
    "    closed=False,\n",
    "    filled=False,\n",
    ")\n",
    "\n",
    "# A closed, filled polygon with a label\n",
    "polyline2 = fo.Polyline(\n",
    "    label=\"triangle\", # `label` is analogous to V7 annotation classes / names\n",
    "    points=[[(0.1, 0.1), (0.3, 0.1), (0.3, 0.3)]],\n",
    "    closed=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "print(polyline1)\n",
    "print(polyline2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Views\n",
    "\n",
    "A `DatasetView` is an object containing filtering rules. These objects can be applied to `Dataset` objects to return a subset of filtered data. Dataset views allow sorting, slicing, and searching operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'test' to '/Users/umbertodifabrizio/fiftyone/cifar10/test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 623k/170M [00:11<53:11, 53.2kB/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfiftyone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfob\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfiftyone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ViewField \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfoz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_zoo_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m cats \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmatch(F(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth.label\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Compute the uniqueness of samples\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/fiftyone/zoo/datasets/__init__.py:335\u001b[0m, in \u001b[0;36mload_zoo_dataset\u001b[0;34m(name_or_url, split, splits, label_field, dataset_name, download_if_necessary, drop_existing_dataset, persistent, overwrite, cleanup, progress, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m         download_kwargs, _ \u001b[38;5;241m=\u001b[39m fou\u001b[38;5;241m.\u001b[39mextract_kwargs_for_class(\n\u001b[1;32m    332\u001b[0m             zoo_dataset_cls, kwargs\n\u001b[1;32m    333\u001b[0m         )\n\u001b[0;32m--> 335\u001b[0m     info, dataset_dir \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_zoo_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_or_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcleanup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     zoo_dataset \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mget_zoo_dataset()\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/fiftyone/zoo/datasets/__init__.py:215\u001b[0m, in \u001b[0;36mdownload_zoo_dataset\u001b[0;34m(name_or_url, split, splits, overwrite, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     _overwrite_download(name_or_url, split\u001b[38;5;241m=\u001b[39msplit, splits\u001b[38;5;241m=\u001b[39msplits)\n\u001b[1;32m    212\u001b[0m zoo_dataset, dataset_dir \u001b[38;5;241m=\u001b[39m _parse_dataset_details(\n\u001b[1;32m    213\u001b[0m     name_or_url, overwrite\u001b[38;5;241m=\u001b[39moverwrite, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    214\u001b[0m )\n\u001b[0;32m--> 215\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mzoo_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcleanup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m info, dataset_dir\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/fiftyone/zoo/datasets/__init__.py:1306\u001b[0m, in \u001b[0;36mZooDataset.download_and_prepare\u001b[0;34m(self, dataset_dir, split, splits, cleanup)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading split \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1297\u001b[0m         split,\n\u001b[1;32m   1298\u001b[0m         split_dir,\n\u001b[1;32m   1299\u001b[0m         suffix,\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1302\u001b[0m (\n\u001b[1;32m   1303\u001b[0m     dataset_type,\n\u001b[1;32m   1304\u001b[0m     num_samples,\n\u001b[1;32m   1305\u001b[0m     classes,\n\u001b[0;32m-> 1306\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscratch_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;66;03m# Add split to ZooDatasetInfo\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/fiftyone/zoo/datasets/torch.py:193\u001b[0m, in \u001b[0;36mCIFAR10Dataset._download_and_prepare\u001b[0;34m(self, dataset_dir, scratch_dir, split)\u001b[0m\n\u001b[1;32m    191\u001b[0m get_classes_fcn \u001b[38;5;241m=\u001b[39m _parse_classification_labels\n\u001b[1;32m    192\u001b[0m sample_parser \u001b[38;5;241m=\u001b[39m foud\u001b[38;5;241m.\u001b[39mImageClassificationSampleParser()\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscratch_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_fcn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_classes_fcn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/fiftyone/zoo/datasets/torch.py:524\u001b[0m, in \u001b[0;36m_download_and_prepare\u001b[0;34m(dataset_dir, scratch_dir, download_fcn, get_classes_fcn, sample_parser)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_download_and_prepare\u001b[39m(\n\u001b[1;32m    517\u001b[0m     dataset_dir,\n\u001b[1;32m    518\u001b[0m     scratch_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m ):\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# Download the torchvision dataset, if necessary\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_fcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscratch_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m     classes \u001b[38;5;241m=\u001b[39m get_classes_fcn(dataset)\n\u001b[1;32m    527\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/fiftyone/zoo/datasets/torch.py:187\u001b[0m, in \u001b[0;36mCIFAR10Dataset._download_and_prepare.<locals>.download_fcn\u001b[0;34m(download_dir)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_fcn\u001b[39m(download_dir):\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/torchvision/datasets/cifar.py:66\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/torchvision/datasets/cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/torchvision/datasets/utils.py:391\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    389\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 391\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    394\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/torchvision/datasets/utils.py:130\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/darwin_fiftyone/.venv/lib/python3.9/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n",
    "cats = dataset.match(F(\"ground_truth.label\") == \"cat\")\n",
    "\n",
    "# Compute the uniqueness of samples\n",
    "fob.compute_uniqueness(cats)\n",
    "\n",
    "# Create the `similar_cats` DatasetView, sorting by uniqueness\n",
    "similar_cats = cats.sort_by(\"uniqueness\", reverse=False)\n",
    "\n",
    "# Display the filtered dataset (DatasetView) in the app\n",
    "session = fo.launch_app(view=similar_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Datasets\n",
    "\n",
    "A grouped dataset allows multiple \"slices\" of samples of possibly different modalities to be displayed simultaneously by organizing them into groups. Groups are quite analogous to multi-slotted items in V7.\n",
    "\n",
    "Creating a grouped dataset is slightly different from creating a regular dataset. Here's an example of how to create and work with a grouped dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "groups = [\"left\", \"center\", \"right\"] # Similar to slot names\n",
    "d = foz.load_zoo_dataset(\"quickstart\")\n",
    "four.random_split(d, {g: 1 / len(groups) for g in groups})\n",
    "filepaths = [d.match_tags(g).values(\"filepath\") for g in groups]\n",
    "filepaths = [dict(zip(groups, fps)) for fps in zip(*filepaths)]\n",
    "\n",
    "# Display the structure of the grouped data\n",
    "print(filepaths[:2])\n",
    "\n",
    "# Create a dataset and declare a group with `add_group_field(group_name)`\n",
    "dataset = fo.Dataset(\"test-grouped-dataset\")\n",
    "dataset.add_group_field(\"group\", default=\"center\") # The optional default parameter specifies the slice of samples that will be returned via the API or visualized in the App’s grid view by default. If you don’t specify a default, one will be inferred from the first sample you add to the dataset.\n",
    " \n",
    "samples = []\n",
    "for fps in filepaths: # For every set of filepaths, create a group (Can be thought of as multi-slotted item)\n",
    "    group = fo.Group()\n",
    "    for name, filepath in fps.items(): # For every sample in the group, append the sample to the group\n",
    "        sample = fo.Sample(filepath=filepath, group=group.element(name)) \n",
    "        samples.append(sample)\n",
    "\n",
    "dataset.add_samples(samples)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a grouped dataset with two groups. Each group contains samples of different media types (image and video in the first group, image and audio in the second group). This structure allows for flexible organization of related samples, similar to multi-slotted items in V7.\n",
    "\n",
    "Grouped datasets are particularly useful when working with related data of different modalities or when you need to maintain relationships between multiple samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
